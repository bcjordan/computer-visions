{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2: Text Generative Models\n",
    "\n",
    "In this assignment we will see some generative models for text: CharRNN, Transformers and Chatbots. Training text models is very time consuming, and uses a ton of data. The really good models also tend to be very large, so we will stick to pretrained models. Those can still be excellent to generate totally new text!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "\n",
    "Embeddings are numeric representations for non-numeric data. In our case we look for embeddings for words. A simple kind of embedding is One-Hot Encoding, where we put a `1` in a vector of all `0`s at the index of the word in the vocabulary.\n",
    "\n",
    "<img src=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/images/one-hot.png?raw=1\" width=\"50%\"/>\n",
    "\n",
    "But that can be very wasteful and also doesn't encode any relationship between the words.\n",
    "\n",
    "To learn semantic relationship a few unsupervised algorithms were proposed. In class we've discussed Continuous Bag of Words and Skip-Gram. Essentially these will mask out part of a sentence and ask the model to predict the missing part. This way the model learns about the context words are used in sentences as well as relationships.\n",
    "\n",
    "Embedding for a word is a vector of numbers:\n",
    "\n",
    "<img src=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/images/embedding2.png?raw=1\" width=\"50%\" />\n",
    "\n",
    "Luckily many of the world leaders in natural language processing have pretrained word embeddings learned on huge corpora, so we don't have to do it ourselves.\n",
    "\n",
    "Allison Parrish of NYU showed some very interesting uses for word embeddings for poetry generation: https://www.youtube.com/watch?v=L3D0JEA1Jdc breeze through this StrangeLoop talk for inspiration. I encourage you to try these methods towards you own generative work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`chakin` is a helper tool for downloading pretrained embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q chakin progressbar2 textgenrnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chakin\n",
    "import progressbar\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the available models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Name  Dimension                     Corpus VocabularySize  \\\n",
      "2          fastText(en)        300                  Wikipedia           2.5M   \n",
      "11         GloVe.6B.50d         50  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "12        GloVe.6B.100d        100  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "13        GloVe.6B.200d        200  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "14        GloVe.6B.300d        300  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "15       GloVe.42B.300d        300          Common Crawl(42B)           1.9M   \n",
      "16      GloVe.840B.300d        300         Common Crawl(840B)           2.2M   \n",
      "17    GloVe.Twitter.25d         25               Twitter(27B)           1.2M   \n",
      "18    GloVe.Twitter.50d         50               Twitter(27B)           1.2M   \n",
      "19   GloVe.Twitter.100d        100               Twitter(27B)           1.2M   \n",
      "20   GloVe.Twitter.200d        200               Twitter(27B)           1.2M   \n",
      "21  word2vec.GoogleNews        300          Google News(100B)           3.0M   \n",
      "\n",
      "      Method Language    Author  \n",
      "2   fastText  English  Facebook  \n",
      "11     GloVe  English  Stanford  \n",
      "12     GloVe  English  Stanford  \n",
      "13     GloVe  English  Stanford  \n",
      "14     GloVe  English  Stanford  \n",
      "15     GloVe  English  Stanford  \n",
      "16     GloVe  English  Stanford  \n",
      "17     GloVe  English  Stanford  \n",
      "18     GloVe  English  Stanford  \n",
      "19     GloVe  English  Stanford  \n",
      "20     GloVe  English  Stanford  \n",
      "21  word2vec  English    Google  \n"
     ]
    }
   ],
   "source": [
    "chakin.search('English')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download GLoVE embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100% ||                                      | Time:  0:06:30   2.1 MiB/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./glove.6B.zip'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chakin.download(number=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only need one file (the smallest dimension one):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  glove.6B.zip\n",
      "  inflating: glove.6B.50d.txt        \n"
     ]
    }
   ],
   "source": [
    "!unzip glove.6B.zip glove.6B.50d.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These files contain the embedding values for each word in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862 -0.00066023 -0.6566 0.27843 -0.14767 -0.55677 0.14658 -0.0095095 0.011658 0.10204 -0.12792 -0.8443 -0.12181 -0.016801 -0.33279 -0.1552 -0.23131 -0.19181 -1.8823 -0.76746 0.099051 -0.42125 -0.19526 4.0071 -0.18594 -0.52287 -0.31681 0.00059213 0.0074449 0.17778 -0.15897 0.012041 -0.054223 -0.29871 -0.15749 -0.34758 -0.045637 -0.44251 0.18785 0.0027849 -0.18411 -0.11514 -0.78581\n",
      ", 0.013441 0.23682 -0.16899 0.40951 0.63812 0.47709 -0.42852 -0.55641 -0.364 -0.23938 0.13001 -0.063734 -0.39575 -0.48162 0.23291 0.090201 -0.13324 0.078639 -0.41634 -0.15428 0.10068 0.48891 0.31226 -0.1252 -0.037512 -1.5179 0.12612 -0.02442 -0.042961 -0.28351 3.5416 -0.11956 -0.014533 -0.1499 0.21864 -0.33412 -0.13872 0.31806 0.70358 0.44858 -0.080262 0.63003 0.32111 -0.46765 0.22786 0.36034 -0.37818 -0.56657 0.044691 0.30392\n",
      ". 0.15164 0.30177 -0.16763 0.17684 0.31719 0.33973 -0.43478 -0.31086 -0.44999 -0.29486 0.16608 0.11963 -0.41328 -0.42353 0.59868 0.28825 -0.11547 -0.041848 -0.67989 -0.25063 0.18472 0.086876 0.46582 0.015035 0.043474 -1.4671 -0.30384 -0.023441 0.30589 -0.21785 3.746 0.0042284 -0.18436 -0.46209 0.098329 -0.11907 0.23919 0.1161 0.41705 0.056763 -6.3681e-05 0.068987 0.087939 -0.10285 -0.13931 0.22314 -0.080803 -0.35652 0.016413 0.10216\n",
      "of 0.70853 0.57088 -0.4716 0.18048 0.54449 0.72603 0.18157 -0.52393 0.10381 -0.17566 0.078852 -0.36216 -0.11829 -0.83336 0.11917 -0.16605 0.061555 -0.012719 -0.56623 0.013616 0.22851 -0.14396 -0.067549 -0.38157 -0.23698 -1.7037 -0.86692 -0.26704 -0.2589 0.1767 3.8676 -0.1613 -0.13273 -0.68881 0.18444 0.0052464 -0.33874 -0.078956 0.24185 0.36576 -0.34727 0.28483 0.075693 -0.062178 -0.38988 0.22902 -0.21617 -0.22562 -0.093918 -0.80375\n",
      "to 0.68047 -0.039263 0.30186 -0.17792 0.42962 0.032246 -0.41376 0.13228 -0.29847 -0.085253 0.17118 0.22419 -0.10046 -0.43653 0.33418 0.67846 0.057204 -0.34448 -0.42785 -0.43275 0.55963 0.10032 0.18677 -0.26854 0.037334 -2.0932 0.22171 -0.39868 0.20912 -0.55725 3.8826 0.47466 -0.95658 -0.37788 0.20869 -0.32752 0.12751 0.088359 0.16351 -0.21634 -0.094375 0.018324 0.21048 -0.03088 -0.19722 0.082279 -0.09434 -0.073297 -0.064699 -0.26044\n"
     ]
    }
   ],
   "source": [
    "!head -5 glove.6B.50d.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load them into memory and organize a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vec_lines = open('glove.6B.50d.txt','rt', encoding='utf-8').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (100000 of 100000) |################| Elapsed Time: 0:00:01 Time:  0:00:01\n"
     ]
    }
   ],
   "source": [
    "w2v_emb_dict = dict()\n",
    "pbar = progressbar.ProgressBar(max_value=100000)\n",
    "for i,l in enumerate(w2vec_lines[1:100000]):\n",
    "    w,emb = l.split(' ', 1)\n",
    "    w2v_emb_dict[w] = np.fromstring(emb, sep=' ')\n",
    "    pbar.update(i+1)\n",
    "pbar.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These would be the first most commonly used tokens in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',', '.', 'of', 'to', 'and', 'in', 'a', '\"', \"'s\", 'for']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(w2v_emb_dict.keys())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Analogies and Similarities\n",
    "\n",
    "Embeddings carry semantic information in their numeric encoding. Exploring this semantic space can be fun, for example looking for similarities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity is measuring the angle between vectors. \n",
    "\n",
    "<img src=\"https://miro.medium.com/max/2432/1*Acs3Kbrrrb4d3fqMlGhMcQ.png\"/>\n",
    "\n",
    "Our embeddings are normalized vectors so looking at the angle between two vectors reveals how far away they are from one another in the high-dimensional embdding space:\n",
    "\n",
    "<img src=\"https://www.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/2b4a7a82-ad4c-4b2a-b808-e423a334de6f.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "w2v_emb_dict_keys = list(w2v_emb_dict.keys())\n",
    "w2v_emb_dict_values = np.array(list(w2v_emb_dict.values()))\n",
    "\n",
    "def find_nearest(w):\n",
    "    return w2v_emb_dict_keys[cosine_similarity(w2v_emb_dict[w].reshape(1,-1), w2v_emb_dict_values)[0].argsort()[-2]]\n",
    "def find_nearest_top_k(v, k=5):\n",
    "    return [w2v_emb_dict_keys[w] for w in cosine_similarity(v.reshape(1,-1), w2v_emb_dict_values)[0].argsort()[-k:].tolist()[::-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by looking at closest neighbors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'france'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest('paris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bigger'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest('big')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'goodbye'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'teaching'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest('learning')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's consider \"**word analogies**\", e.g. completing the sentence: \"Paris is to France like Rome is to ___\" \\(Italy\\)\n",
    "\n",
    "To explain this geometrically:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/2632/1*EOVxNmHkrsPQ7Q44N0OiQg.png\" width=\"60%\" />\n",
    "\n",
    "The offset vector between \"paris\" and \"france\" is the \"Captial of\" vector, and when we apply it to \"rome\" we expect to get \"italy\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['italy', 'spain', 'rome', 'portugal', 'france']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest_top_k(w2v_emb_dict['france'] - w2v_emb_dict['paris'] + w2v_emb_dict['rome'], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['king', 'queen', 'daughter', 'prince', 'throne']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest_top_k(w2v_emb_dict['king'] - w2v_emb_dict['man'] + w2v_emb_dict['woman'], 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following analogies:\n",
    "1. sushi-rice is like pizza-___\n",
    "2. sushi-rice is like steak-___\n",
    "3. shirt-clothing is like phone-___\n",
    "4. shirt-clothing is like bowl-___\n",
    "5. book-reading is like TV-___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to find analogies that don't work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Char RNN\n",
    "\n",
    "CharRNN is a simple recurrent neural network architecture that works on the character level (not words). It's surprisingly powerful at generating text. These were popularized by [Andrej Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/).\n",
    "\n",
    "<img src=\"http://karpathy.github.io/assets/rnn/charseq.jpeg\" width=\"50%\"/>\n",
    "\n",
    "The `textgenrnn` package is a convnient way to train and generate with CharRNNs. Here we're using its built in model. They have multiple models [published](https://github.com/minimaxir/textgenrnn/tree/master/weights) trained on different corpora.\n",
    "\n",
    "People created some very cool projects with it: https://github.com/minimaxir/textgenrnn#projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was a little time when she was realized to make a Hillary Clinton\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from textgenrnn import textgenrnn\n",
    "\n",
    "textgen = textgenrnn()\n",
    "textgen.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can supply a prefix to prime the model with text to complete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When life gives you lemons and tell me a lot of your life and it reading it was leaving it.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textgen.generate(prefix=\"When life gives you lemons \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also let the model try different \"temperatures\". The \"temperature\" controls the level of random choice when picking the next character, instead of always the most likely one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "[Specific] Can someone please remove the subreddit and why do you guys think about the story of the state of the state of the state of the state of the same card should be a computer and why is this here?\n",
      "\n",
      "So it was a boy and I was a good mission of the most poster to see this back the person to see the start of the story of the state of the best friends and don't want to see them and the subreddit is the story of the story of the state of the state of the state of the state of the same community is \n",
      "\n",
      "[PS4] LFG and Mexico Stream Is A Bear That In The Same I Continue Of Starting a Popular Progress And And Start To Be Comeding For The Game of The World That End Me In The Same State of Anything\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "The Research Office Special Media Discussion\n",
      "\n",
      "I can't see a super month connection of the sidewalk book with a better popular of the end of a specialist in Mark Carrot?\n",
      "\n",
      "I was a planet community today. All it would be a beautiful car?\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "No one 4 sense more appears on Holiday, giannixiar,, shouldn't they go quest in cheeses, instead of puppy\n",
      "\n",
      "I don't late redey videos\n",
      "\n",
      "Uneverwifi taken their foot on it's ‘do not to have taken game with suitabo\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textgen.generate_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Try different prefixes and temperatures. (examine the `.generate()` function, by running a cell with `textgenrnn.generate?`)\n",
    "* Try a different pretrained model from `textgenrnn`\n",
    "* Advanced: train your own model! `textgenrnn` provide a **very** simple mechanism to do so: https://github.com/minimaxir/textgenrnn#examples, you just need to supply a text file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "Transformers are relative newcomers to the language processing world. They are an evolution of recurrent neural networks and activation layers. Using transformers has increased the capability of generating believable text by a whole lot, so much so that [ethical issues](https://www.theverge.com/2019/2/21/18234500/ai-ethics-debate-researchers-harmful-programs-openai) have arised around release of models or restrictive use of them.\n",
    "\n",
    "Architecture wise, transformers are an encoder-decoder scheme that relies heavily on \"attention\" - a mechanism that allows every step to examine both past and future.\n",
    "\n",
    "<img src=\"http://lilianweng.github.io/lil-log/assets/images/transformer.png\" />\n",
    "\n",
    "One recent model from OpenAI is GPT-2, which is freely available for download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When your user gives you lemons you generate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", it might not require much, but it can definitely have its benefits\n"
     ]
    }
   ],
   "source": [
    "!python ./transformers/examples/run_generation.py \\\n",
    "    --model_type=gpt2 \\\n",
    "    --length=200 \\\n",
    "    --model_name_or_path=gpt2 \\\n",
    "    --stop_token=\".\" \\\n",
    "    --prompt=\"When life gives you lemons\" 2>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\n",
      "\n",
      "And suddenly there  was a powerful tidal wave coming out of her ears.\n",
      "\n",
      "'Hey..'\n",
      "\n",
      "Professor McGonagall shook her head with a glance at Goyle,\n",
      "\n",
      "'Woof! Don't...I'm getting...shit! Shit!'\n",
      "\n",
      "Goyle was covered in flaming hair\n",
      "\n",
      "'Thank god I didn't say anything about it!'\n",
      "\n",
      "Now there was blood flowing from Professor McGonagall's face as she saw. Something yellow oozed out of her nose.\n",
      "\n",
      "Faint red vomit appeared from the spot.\n",
      "\n",
      "'Guzzle! Can you smell...!'\n",
      "\n",
      "Instantly other students quickly followed in dark circles around the teleportation weapon.\n",
      "\n",
      "Instantly darker objects appeared.\n",
      "\n",
      "One of the red objects looked like a shinier one or two scuttlecheeks.\n",
      "\n",
      "The red color drained from Goyle's face.\n",
      "\n",
      "Goyle let out a terror without smiling.\n",
      "\n",
      "But,\n"
     ]
    }
   ],
   "source": [
    "!python ./transformers/examples/run_generation.py \\\n",
    "    --model_type=gpt2 \\\n",
    "    --length=200 \\\n",
    "    --model_name_or_path=gpt2 \\\n",
    "    --prompt=\"Harry witnessed Professor McGonagall walking right past Peeves who \\\n",
    "was determinedly loosening a crystal chandelier and could have sworn he heard her \\\n",
    "tell the poltergeist out of the corner of her mouth, 'It unscrews the other way.’\" 2>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let's see how it does with Williams' \"This is just to say\" (https://poets.org/poem/just-say) poem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", and are later sliced off, for you and those you love to share with others\n"
     ]
    }
   ],
   "source": [
    "!python ./transformers/examples/run_generation.py \\\n",
    "    --model_type=gpt2 \\\n",
    "    --length=50 \\\n",
    "    --model_name_or_path=gpt2 \\\n",
    "    --stop_token=\".\" \\\n",
    "    --prompt=\"I have eaten the plums \\\n",
    "that were in the icebox \\\n",
    "and which you were probably \\\n",
    "saving for breakfast\" 2>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(this BTW is one of my favorite poems ever. So sweet and so plain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Try different prefix inputs\n",
    "* Try different temperatures with the `--temperature` argument.\n",
    "* Advanced: Try a different model than GPT-2.\n",
    "\n",
    "On the help section for the generation script you can find all the models:\n",
    "```\n",
    "--model_name_or_path MODEL_NAME_OR_PATH\n",
    "                        Path to pre-trained model or shortcut name selected in\n",
    "                        the list: gpt2, gpt2-medium, gpt2-large, distilgpt2,\n",
    "                        openai-gpt, xlnet-base-cased, xlnet-large-cased,\n",
    "                        transfo-xl-wt103, xlm-mlm-en-2048, xlm-mlm-ende-1024,\n",
    "                        xlm-mlm-enfr-1024, xlm-mlm-enro-1024, xlm-mlm-tlm-\n",
    "                        xnli15-1024, xlm-mlm-xnli15-1024, xlm-clm-enfr-1024,\n",
    "                        xlm-clm-ende-1024, xlm-mlm-17-1280, xlm-mlm-100-1280,\n",
    "                        ctrl\n",
    "```\n",
    "The `ctrl` model is very recent work (from SalesForce research), just from a couple of weeks ago, it's supposed to be really awesome at controling the output text. Be warned - the model is a **6Gb download**! It might be worth it..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatBot\n",
    "\n",
    "[Chatbots](https://en.wikipedia.org/wiki/Chatbot) are conversational AI agents that can respond to text input. It's still ways away from a convincing conversation in general open-ended scenarios, but in certain applications chatbots are a big success, e.g. in the public services industry's online portals.\n",
    "\n",
    "`huggingface` again have released their pretrained models for chatbots based on transformers just a few months ago: https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313#79c5\n",
    "\n",
    "You can also use their online demo: https://convai.huggingface.co/persona/my-only-friend-is-a-dog-i-work-at-a-newspaper-my-father-used-to-be-a-butcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch_transformers in /usr/local/lib/python3.7/site-packages (1.2.0)\n",
      "Collecting pytorch-ignite\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/31/efcc2b587419b1f54c5c6ef51996f91bb5d8f760537d17de674c89e06048/pytorch_ignite-0.2.1-py2.py3-none-any.whl (84kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 2.4MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/site-packages (from pytorch_transformers) (4.36.1)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/site-packages (from pytorch_transformers) (0.0.35)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/site-packages (from pytorch_transformers) (1.17.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/site-packages (from pytorch_transformers) (2.22.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/site-packages (from pytorch_transformers) (0.1.83)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/site-packages (from pytorch_transformers) (1.3.0)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.7/site-packages (from pytorch_transformers) (2019.8.19)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/site-packages (from pytorch_transformers) (1.9.253)\n",
      "Requirement already satisfied: six in /usr/local/Cellar/protobuf/3.10.0/libexec/lib/python3.7/site-packages (from sacremoses->pytorch_transformers) (1.12.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/site-packages (from sacremoses->pytorch_transformers) (7.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/site-packages (from sacremoses->pytorch_transformers) (0.14.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests->pytorch_transformers) (1.25.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests->pytorch_transformers) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests->pytorch_transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests->pytorch_transformers) (2.8)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.253 in /usr/local/lib/python3.7/site-packages (from boto3->pytorch_transformers) (1.12.253)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.7/site-packages (from boto3->pytorch_transformers) (0.2.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/site-packages (from boto3->pytorch_transformers) (0.9.4)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.7/site-packages (from botocore<1.13.0,>=1.12.253->boto3->pytorch_transformers) (2.8.0)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.7/site-packages (from botocore<1.13.0,>=1.12.253->boto3->pytorch_transformers) (0.15.2)\n",
      "Installing collected packages: pytorch-ignite\n",
      "Successfully installed pytorch-ignite-0.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -q pytorch_transformers pytorch-ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'transfer-learning-conv-ai'...\n",
      "remote: Enumerating objects: 87, done.\u001b[K\n",
      "remote: Total 87 (delta 0), reused 0 (delta 0), pack-reused 87\u001b[K\n",
      "Unpacking objects: 100% (87/87), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/huggingface/transfer-learning-conv-ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,threading,subprocess,os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot_proc():\n",
    "    proc = subprocess.Popen([sys.executable, \n",
    "                             os.getcwd()+'/transfer-learning-conv-ai/interact.py'\n",
    "                            ],\n",
    "                            stdout=subprocess.PIPE,\n",
    "                            stdin=subprocess.PIPE,\n",
    "                            stderr=subprocess.DEVNULL)\n",
    "    pout = proc.stdout\n",
    "    pin = proc.stdin\n",
    "    \n",
    "    return proc,pout,pin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb1_proc, cb1_pout, cb1_pin = chatbot_proc(); # create a chatbot process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb1_pin.write(b\"--temperature=1.1\\n\"), cb1_pin.flush();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> hi how are you today?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cb1_pout.readline().decode(sys.stdout.encoding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Talk to your chatbot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> i'm doing well just listening to some music\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cb1_pin.write(b\"i'm doing mighty fine! and how are you?\\n\"), cb1_pin.flush();\n",
    "print(cb1_pout.readline().decode(sys.stdout.encoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> i am listening to a lot of pop music\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cb1_pin.write(b\"no way! i'm also listening to music. what music are you listening to?\\n\"), cb1_pin.flush();\n",
    "print(cb1_pout.readline().decode(sys.stdout.encoding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also quite funny to get it to talk to itself - it never get tired!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb1_output = b\"i am listening to a lot of pop music\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B: yeah, i know what you mean.\n",
      "A: what do you do for a living?\n",
      "B: i am a mechanic.\n",
      "A: i am a pilot.\n",
      "B: what do you do for work?\n",
      "A: i fix planes.\n",
      "B: what kind of planes do you have?\n",
      "A: do you have any hobbies?\n",
      "B: i like to listen to music.\n",
      "A: what kind of music do you like?\n"
     ]
    }
   ],
   "source": [
    "partyA = True\n",
    "for _ in range(10):\n",
    "    partyA = not partyA\n",
    "    cb1_pin.write(cb1_output), cb1_pin.flush();\n",
    "    cb1_output = cb1_pout.readline()[4:]\n",
    "    print(\"%s: %s\" % ('A' if partyA else 'B',\n",
    "          cb1_output[:-1].decode(sys.stdout.encoding)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb1_proc.kill() # kill the chatbot process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Try some different inputs\n",
    "* Advanced: Spin up another chatbot and have them talk to one another (by feeding the outputs across)\n",
    "* Advanced: Use a different underlying model than GPT-2 for your chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "That's a wrap!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
