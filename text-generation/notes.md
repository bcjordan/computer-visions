HW1 - will do face2face, horse to zebra, dog to chicken

# Text Modeling 

Fixed window (only look two back)
- Hard to maintain long-term/distance relationships

Counting: "bag of words"

Continuous bag of words - look before/after as well

Word2Vec - continuous bag of words / skip-gram
    https://towardsdatascience.com/word2vec-skip-gram-model-part-1-intuition-78614e4d6e0b
 
Unsupervised - can train on large dataset (or semi-supervision)

Embedders - takes weeks to train

With Word2Vec, you can use arithmetic to say Beijing - China + Russia = Moscow

# Sequence Modeling / RNNs

CNNs as applied to text

Why not just Feed Forward NN? Heavyweight/ fixed length



# Projects

https://svip-lab.github.io/project/impersonator

GLITCH - generating fashion designs

# Stats

30k + most used words
100k - modern english
3-400k - all



# Quotes

"Life is what happens when you're busy making plans." - John Lennon

"You shall know a \[man|word\] by the company it keeps"- JR Firth
